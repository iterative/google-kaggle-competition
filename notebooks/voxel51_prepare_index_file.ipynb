{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.utils.random as four\n",
    "from fiftyone import ViewField as F\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from subprocess import call\n",
    "\n",
    "from typing import List\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset to local\n",
    "Download the datasets you want to test from our Data registry to your local folder and unzip it. Be careful about the revision of the Data registry.\n",
    "\n",
    "You do not need any credentials to download the files.\n",
    "\n",
    "Data registry link: https://github.com/iterative/google-kaggle-competition-data-pipeline/tree/pipeline_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_name: str, output_path:Path, partitions: List[int]=list(range(0,10))):\n",
    "\n",
    "    (output_path/'data').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Remove old data\n",
    "    shutil.rmtree(output_path/'data')\n",
    "\n",
    "    download_and_unzip_dataset(dataset_name=dataset_name, output_path=output_path, partitions=partitions)\n",
    "    get_labels(dataset_name=dataset_name, output_path=output_path, partitions=partitions)\n",
    "\n",
    "\n",
    "def download_and_unzip_dataset(dataset_name: str, output_path: Path, partitions: List[int]):\n",
    "    partitions_in_output_path = {int(file.name.split(\"partition_\")[1].split(\".\")[0]):file for file in output_path.iterdir() if file.name.startswith(\"partition_\") and file.name.endswith(\".zip\")}\n",
    "\n",
    "     # Download zip files\n",
    "    for partition in partitions:\n",
    "        zip_file_path = output_path/f\"partition_{partition}.zip\"\n",
    "        \n",
    "        print(\"path\", zip_file_path)\n",
    "        # Download zip file if not already present\n",
    "        if partition not in partitions_in_output_path.keys():\n",
    "            call([\"dvc\",\"get\",\"https://github.com/iterative/google-kaggle-competition-data-pipeline\",f\"datasets/{dataset_name}/partition_{partition}.zip\",\"-o\",str(output_path)])            \n",
    "        # Unzip all selected partitions as we have empty 'data'\n",
    "        call([\"unzip\",\"-o\",zip_file_path,\"-d\",output_path])\n",
    "\n",
    "\n",
    "def get_labels(dataset_name: str, output_path: Path, partitions: List[int]):\n",
    "    if (output_path/'labels.json').exists():\n",
    "        (output_path/'labels.json').unlink()\n",
    "\n",
    "    call([\"dvc\",\"get\",\"https://github.com/iterative/google-kaggle-competition-data-pipeline\",f\"datasets/{dataset_name}/labels.json\",\"-o\",str(output_path)])\n",
    "\n",
    "    with open(output_path/'labels.json', 'r') as file:\n",
    "        labels = json.load(file)\n",
    "\n",
    "    imgs_to_keep = [img for img,lbl in labels['labels'].items() if lbl['attributes']['partition'] in partitions]\n",
    "\n",
    "    labels_to_keep = labels['labels'].copy()\n",
    "    for img_name in labels['labels'].keys():\n",
    "        if img_name not in imgs_to_keep:\n",
    "                labels_to_keep.pop(img_name)\n",
    "    labels['labels'] = labels_to_keep\n",
    "\n",
    "    with open(output_path/\"labels.json\", \"w\") as outfile:\n",
    "        json.dump(labels, outfile)\n",
    "        \n",
    "\n",
    "prepare_dataset(dataset_name=\"vision_furniture\", output_path=Path(\"../datasets/vision_furniture/\"), partitions=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets\n",
    "Load datasets that you want to work with into Voxel51."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case, you need to delete the dataset because it was not created properly\n",
    "dataset = fo.load_dataset('food_101_small')\n",
    "dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10100/10100 [6.9s elapsed, 0s remaining, 1.4K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    if not fo.dataset_exists(dataset_name):\n",
    "        dataset = fo.Dataset.from_dir(\n",
    "            dataset_dir=Path(f\"../datasets/{dataset_name}/\"),\n",
    "            dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "            name=dataset_name\n",
    "        )\n",
    "    else:\n",
    "        dataset = fo.load_dataset(dataset_name)\n",
    "    return dataset\n",
    "\n",
    "dataset_food_101_small = load_dataset('food_101_small')\n",
    "dataset_freiburg_groceries = load_dataset('freiburg_groceries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick check that everything was loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n",
      "{'prime_rib': 100, 'hot_and_sour_soup': 100, 'grilled_cheese_sandwich': 100, 'crab_cakes': 100, 'gyoza': 100, 'french_toast': 100, 'fish_and_chips': 100, 'chicken_quesadilla': 100, 'lasagna': 100, 'chicken_curry': 100, 'foie_gras': 100, 'apple_pie': 100, 'carrot_cake': 100, 'macarons': 100, 'french_fries': 100, 'pizza': 100, 'frozen_yogurt': 100, 'donuts': 100, 'dumplings': 100, 'bibimbap': 100, 'lobster_bisque': 100, 'beef_tartare': 100, 'cheese_plate': 100, 'seaweed_salad': 100, 'cup_cakes': 100, 'eggs_benedict': 100, 'steak': 100, 'creme_brulee': 100, 'guacamole': 100, 'grilled_salmon': 100, 'baby_back_ribs': 100, 'tuna_tartare': 100, 'gnocchi': 100, 'onion_rings': 100, 'pulled_pork_sandwich': 100, 'mussels': 100, 'huevos_rancheros': 100, 'chicken_wings': 100, 'bread_pudding': 100, 'oysters': 100, 'baklava': 100, 'pad_thai': 100, 'paella': 100, 'deviled_eggs': 100, 'clam_chowder': 100, 'greek_salad': 100, 'hot_dog': 100, 'tacos': 100, 'chocolate_mousse': 100, 'tiramisu': 100, 'panna_cotta': 100, 'falafel': 100, 'macaroni_and_cheese': 100, 'nachos': 100, 'shrimp_and_grits': 100, 'pork_chop': 100, 'fried_calamari': 100, 'risotto': 100, 'ice_cream': 100, 'ravioli': 100, 'ramen': 100, 'spaghetti_bolognese': 100, 'sashimi': 100, 'scallops': 100, 'fried_rice': 100, 'beef_carpaccio': 100, 'club_sandwich': 100, 'cheesecake': 100, 'spring_rolls': 100, 'sushi': 100, 'poutine': 100, 'ceviche': 100, 'garlic_bread': 100, 'chocolate_cake': 100, 'edamame': 100, 'lobster_roll_sandwich': 100, 'caprese_salad': 100, 'pancakes': 100, 'samosa': 100, 'breakfast_burrito': 100, 'hummus': 100, 'cannoli': 100, 'takoyaki': 100, 'waffles': 100, 'miso_soup': 100, 'croque_madame': 100, 'hamburger': 100, 'caesar_salad': 100, 'churros': 100, 'peking_duck': 100, 'escargots': 100, 'beet_salad': 100, 'pho': 100, 'spaghetti_carbonara': 100, 'strawberry_shortcake': 100, 'red_velvet_cake': 100, 'beignets': 100, 'omelette': 100, 'french_onion_soup': 100, 'filet_mignon': 100, 'bruschetta': 100}\n",
      "4947\n",
      "{'NUTS': 168, 'BEANS': 136, 'CEREAL': 278, 'COFFEE': 298, 'RICE': 150, 'SUGAR': 118, 'VINEGAR': 157, 'TEA': 283, 'CAKE': 161, 'JUICE': 302, 'CHIPS': 181, 'MILK': 162, 'CANDY': 372, 'JAM': 241, 'PASTA': 172, 'TOMATO_SAUCE': 171, 'CORN': 97, 'FLOUR': 109, 'OIL': 143, 'SPICES': 207, 'CHOCOLATE': 307, 'WATER': 262, 'HONEY': 185, 'SODA': 177, 'FISH': 110}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_food_101_small.count())\n",
    "print(dataset_food_101_small.count_values('ground_truth.label'))\n",
    "\n",
    "print(dataset_freiburg_groceries.count())\n",
    "print(dataset_freiburg_groceries.count_values('ground_truth.label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and changing of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_food_101 = (\n",
    "    dataset_food_101_small\n",
    "    .map_labels(\"ground_truth\", {\"spaghetti_carbonara\": \"spaghetti\", \"spaghetti_bolognese\": \"spaghetti\"})\n",
    ")\n",
    "\n",
    "view_only_pizza = (\n",
    "    view_food_101\n",
    "    .filter_labels(\"ground_truth\", F(\"label\").is_in(['pizza']))\n",
    ")\n",
    "\n",
    "# This function replaces original label in ground_truth.label with new label. This means that all images will have the same single label.\n",
    "view_freiburg = (\n",
    "    dataset_freiburg_groceries\n",
    "    .set_field(\"ground_truth.label\", \"packaged_goods\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = view_food_101.clone()\n",
    "dataset.merge_samples(view_freiburg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chocolate_mousse': 100, 'tiramisu': 100, 'cheesecake': 100, 'fried_rice': 100, 'sushi': 100, 'poutine': 100, 'spring_rolls': 100, 'ceviche': 100, 'ravioli': 100, 'packaged_goods': 4947, 'club_sandwich': 100, 'scallops': 100, 'ramen': 100, 'sashimi': 100, 'beef_carpaccio': 100, 'samosa': 100, 'hummus': 100, 'breakfast_burrito': 100, 'spaghetti': 200, 'takoyaki': 100, 'cannoli': 100, 'garlic_bread': 100, 'edamame': 100, 'chocolate_cake': 100, 'caprese_salad': 100, 'lobster_roll_sandwich': 100, 'pancakes': 100, 'croque_madame': 100, 'caesar_salad': 100, 'hamburger': 100, 'churros': 100, 'peking_duck': 100, 'escargots': 100, 'miso_soup': 100, 'waffles': 100, 'omelette': 100, 'french_onion_soup': 100, 'filet_mignon': 100, 'bruschetta': 100, 'prime_rib': 100, 'pho': 100, 'beet_salad': 100, 'strawberry_shortcake': 100, 'red_velvet_cake': 100, 'beignets': 100, 'foie_gras': 100, 'pizza': 100, 'carrot_cake': 100, 'macarons': 100, 'french_fries': 100, 'hot_and_sour_soup': 100, 'grilled_cheese_sandwich': 100, 'crab_cakes': 100, 'gyoza': 100, 'french_toast': 100, 'chicken_curry': 100, 'apple_pie': 100, 'fish_and_chips': 100, 'grilled_salmon': 100, 'cup_cakes': 100, 'eggs_benedict': 100, 'steak': 100, 'creme_brulee': 100, 'guacamole': 100, 'chicken_quesadilla': 100, 'lasagna': 100, 'frozen_yogurt': 100, 'lobster_bisque': 100, 'dumplings': 100, 'donuts': 100, 'tuna_tartare': 100, 'bibimbap': 100, 'seaweed_salad': 100, 'beef_tartare': 100, 'cheese_plate': 100, 'chicken_wings': 100, 'bread_pudding': 100, 'baby_back_ribs': 100, 'pad_thai': 100, 'oysters': 100, 'paella': 100, 'deviled_eggs': 100, 'gnocchi': 100, 'baklava': 100, 'pulled_pork_sandwich': 100, 'greek_salad': 100, 'onion_rings': 100, 'mussels': 100, 'huevos_rancheros': 100, 'falafel': 100, 'panna_cotta': 100, 'nachos': 100, 'shrimp_and_grits': 100, 'macaroni_and_cheese': 100, 'pork_chop': 100, 'clam_chowder': 100, 'fried_calamari': 100, 'hot_dog': 100, 'risotto': 100, 'ice_cream': 100, 'tacos': 100}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.count_values('ground_truth.label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case you would like to manually inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save changes you made in the UI, you need to call\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "Note: We do not want to do split here. We need to move it to data pipeline to make sure that ML and Search-index pipeline work with different set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: This produces splits that are not overlapping\n",
    "view_train,view_val,view_test = four.random_split(dataset, [0.6,0.2,0.2], seed=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataset\n",
    "Note: Beware that there are absolute paths in manifest.json. Unfortunately, relative paths are buggy and do not work well in fiftyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 9028/9028 [3.6s elapsed, 0s remaining, 3.0K samples/s]        \n",
      " 100% |███████████████| 3010/3010 [1.2s elapsed, 0s remaining, 2.6K samples/s]         \n",
      " 100% |███████████████| 3009/3009 [1.1s elapsed, 0s remaining, 2.7K samples/s]         \n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"../data/voxel51/\")\n",
    "\n",
    "view_train.export(export_dir=str(output_dir/'train'), \n",
    "                dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "                label_field='ground_truth',\n",
    "                data_path='manifest.json',\n",
    "                labels_path='labels.json',\n",
    "                export_media='manifest',\n",
    "                overwrite=True\n",
    "                )\n",
    "\n",
    "view_val.export(export_dir=str(output_dir/'val'), \n",
    "                dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "                label_field='ground_truth',\n",
    "                data_path='manifest.json',\n",
    "                labels_path='labels.json',\n",
    "                export_media='manifest',\n",
    "                overwrite=True\n",
    "                )\n",
    "\n",
    "view_test.export(export_dir=str(output_dir/'test'), \n",
    "                dataset_type=fo.types.FiftyOneImageClassificationDataset,\n",
    "                label_field='ground_truth',\n",
    "                data_path='manifest.json',\n",
    "                labels_path='labels.json',\n",
    "                export_media='manifest',\n",
    "                overwrite=True\n",
    "                )     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that changes absolute paths in manifest.json to relative paths\n",
    "This is useful in case you would like to share this dataset with someone else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../data/voxel51/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import relpath\n",
    "from pathlib import Path\n",
    "\n",
    "def create_relative_paths(manifest_path: Path):\n",
    "    manifest_path_parent = manifest_path.absolute().parent\n",
    "\n",
    "    with open(manifest_path) as json_file:\n",
    "        manifest = json.load(json_file)\n",
    "\n",
    "    # Relative path should look like as follows: \"<dataset_name>/data/img_name_with_extension\"\n",
    "    manifest_relative = {img_name:\"/\".join(Path(abs_path).parts[-3:]) for img_name, abs_path in manifest.items()}\n",
    "    #manifest_relative = {img_name:relpath(Path(abs_path), manifest_path_parent) for img_name, abs_path in manifest.items()}\n",
    "\n",
    "    with open(manifest_path_parent/'manifest_relative.json', \"w\") as outfile:\n",
    "        json.dump(manifest_relative, outfile)\n",
    "\n",
    "\n",
    "create_relative_paths(manifest_path=output_dir/\"train\"/\"manifest.json\")\n",
    "create_relative_paths(manifest_path=output_dir/\"val\"/\"manifest.json\")\n",
    "create_relative_paths(manifest_path=output_dir/\"test\"/\"manifest.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that changes relative paths in manifest_relative.json to absolute paths\n",
    "This is useful in case you would like to share this dataset with someone else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input should be dictionary with format: {\"<dataset_name>: <absolute_path_to_the_folder>\"}\n",
    "# For example: {\"food_101_small\": \"/workspaces/google-kaggle-competition/datasets/food_101_small\"}\n",
    "\n",
    "def create_absolute_paths(manifest_path: Path, dataset_path_dic):\n",
    "    manifest_path=output_dir/\"train\"/\"manifest_relative.json\"\n",
    "    manifest_path_parent = manifest_path.absolute().parent\n",
    "\n",
    "    with open(manifest_path) as json_file:\n",
    "        manifest_relative = json.load(json_file)\n",
    "\n",
    "    manifest_absolute = {}\n",
    "    for img_name, rel_path in manifest_relative.items():\n",
    "        rel_path = Path(rel_path)\n",
    "        dataset_name = Path(rel_path).parts[0]\n",
    "        abs_path = Path(dataset_path_dic[dataset_name])/rel_path.parts[1]/rel_path.parts[2]\n",
    "        manifest_absolute[img_name] = str(abs_path)\n",
    "    \n",
    "    with open(manifest_path_parent/'manifest.json', \"w\") as outfile:\n",
    "        json.dump(manifest_absolute, outfile)\n",
    "\n",
    "\n",
    "\n",
    "dataset_path_dic = {}\n",
    "dataset_path_dic[\"food_101_small\"] = \"/workspaces/google-kaggle-competition/datasets/food_101_small\"\n",
    "dataset_path_dic[\"freiburg_groceries\"] = \"/workspaces/google-kaggle-competition/datasets/freiburg_groceries\"\n",
    "\n",
    "create_absolute_paths(manifest_path=output_dir/\"train\"/\"manifest.json\", dataset_path_dic=dataset_path_dic)\n",
    "create_absolute_paths(manifest_path=output_dir/\"val\"/\"manifest.json\", dataset_path_dic=dataset_path_dic)\n",
    "create_absolute_paths(manifest_path=output_dir/\"test\"/\"manifest.json\", dataset_path_dic=dataset_path_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to import it to PyTorch\n",
    "Once you generate labels.json and manifest.json files, you can load them into PyTorch with Custom data loader as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FiftyOneTorchDataset(Path(\"/data/voxel51/train\"), transform=ToTensor())\n",
    "val_dataset = FiftyOneTorchDataset(Path(\"/data/voxel51/val\"), transform=ToTensor())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('google-kaggle-TDiTHuYo-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e704d1b9a7e018aa4e7ab1d2650c1618698ef2d7b90d5327c0dfafc5a7a2f30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
